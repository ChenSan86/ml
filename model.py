"""
æ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼šåŒå¡”æ¶æ„ + å¤šå±‚äº¤äº’
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from config import config


class SiameseEncoder(nn.Module):
    """å­ªç”Ÿç¼–ç å™¨ï¼šä½¿ç”¨BiLSTMç¼–ç æ–‡æœ¬"""
    
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, dropout):
        super(SiameseEncoder, self).__init__()
        
        # Embeddingå±‚
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        
        # BiLSTMå±‚
        self.lstm = nn.LSTM(
            embed_dim, 
            hidden_dim // 2,  # åŒå‘ï¼Œæ‰€ä»¥hidden_dimè¦é™¤ä»¥2
            num_layers=num_layers,
            batch_first=True,
            bidirectional=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        """
        x: [batch_size, seq_len]
        return: [batch_size, hidden_dim]
        """
        # Embedding: [batch_size, seq_len, embed_dim]
        embedded = self.embedding(x)
        embedded = self.dropout(embedded)
        
        # LSTM: output [batch_size, seq_len, hidden_dim]
        lstm_out, (hidden, cell) = self.lstm(embedded)
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º (æˆ–è€…å¯ä»¥ç”¨mean pooling)
        # hidden: [num_layers*2, batch_size, hidden_dim//2]
        # å–æœ€åä¸€å±‚çš„æ­£å‘å’Œåå‘hidden stateæ‹¼æ¥
        forward_hidden = hidden[-2, :, :]  # æœ€åä¸€å±‚æ­£å‘
        backward_hidden = hidden[-1, :, :] # æœ€åä¸€å±‚åå‘
        
        # æ‹¼æ¥: [batch_size, hidden_dim]
        sentence_repr = torch.cat([forward_hidden, backward_hidden], dim=1)
        
        return sentence_repr, lstm_out


class InteractionLayer(nn.Module):
    """äº¤äº’å±‚ï¼šè®¡ç®—ä¸¤ä¸ªå¥å­è¡¨ç¤ºä¹‹é—´çš„å¤šç§äº¤äº’ç‰¹å¾"""
    
    def __init__(self, hidden_dim):
        super(InteractionLayer, self).__init__()
        
    def forward(self, repr1, repr2):
        """
        è®¡ç®—å¤šç§äº¤äº’ç‰¹å¾
        repr1, repr2: [batch_size, hidden_dim]
        return: [batch_size, feature_dim]
        """
        # 1. Cosine similarity
        cos_sim = F.cosine_similarity(repr1, repr2, dim=1, eps=1e-8)
        
        # 2. Element-wise product
        element_product = repr1 * repr2
        
        # 3. Element-wise difference
        element_diff = torch.abs(repr1 - repr2)
        
        # 4. Element-wise sum
        element_sum = repr1 + repr2
        
        # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
        # [batch_size, hidden_dim * 3 + 1]
        interaction_features = torch.cat([
            cos_sim.unsqueeze(1),
            element_product,
            element_diff,
            element_sum
        ], dim=1)
        
        return interaction_features


class TextMatchModel(nn.Module):
    """å®Œæ•´çš„æ–‡æœ¬åŒ¹é…æ¨¡å‹"""
    
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, dropout):
        super(TextMatchModel, self).__init__()
        
        # å…±äº«çš„å­ªç”Ÿç¼–ç å™¨
        self.encoder = SiameseEncoder(vocab_size, embed_dim, hidden_dim, num_layers, dropout)
        
        # äº¤äº’å±‚
        self.interaction = InteractionLayer(hidden_dim)
        
        # åˆ†ç±»å±‚
        interaction_dim = hidden_dim * 3 + 1
        self.classifier = nn.Sequential(
            nn.Linear(interaction_dim, 512),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(256, 1)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                nn.init.uniform_(module.weight, -0.1, 0.1)
                if module.padding_idx is not None:
                    module.weight.data[module.padding_idx].zero_()
    
    def forward(self, query1, query2):
        """
        query1, query2: [batch_size, seq_len]
        return: [batch_size, 1]
        """
        # ç¼–ç ä¸¤ä¸ªå¥å­
        repr1, _ = self.encoder(query1)
        repr2, _ = self.encoder(query2)
        
        # è®¡ç®—äº¤äº’ç‰¹å¾
        interaction_features = self.interaction(repr1, repr2)
        
        # åˆ†ç±»
        logits = self.classifier(interaction_features)
        
        return logits.squeeze(-1)  # [batch_size]


def create_model():
    """åˆ›å»ºæ¨¡å‹å®ä¾‹"""
    model = TextMatchModel(
        vocab_size=config.VOCAB_SIZE,
        embed_dim=config.EMBED_DIM,
        hidden_dim=config.HIDDEN_DIM,
        num_layers=config.NUM_LAYERS,
        dropout=config.DROPOUT
    )
    
    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"\nğŸ—ï¸  æ¨¡å‹åˆ›å»ºå®Œæˆ:")
    print(f"   æ€»å‚æ•°é‡: {total_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    return model


if __name__ == '__main__':
    # æµ‹è¯•æ¨¡å‹
    model = create_model()
    model.to(config.DEVICE)
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size = 4
    seq_len = 60
    query1 = torch.randint(1, 1000, (batch_size, seq_len)).to(config.DEVICE)
    query2 = torch.randint(1, 1000, (batch_size, seq_len)).to(config.DEVICE)
    
    outputs = model(query1, query2)
    print(f"\nğŸ” æ¨¡å‹æµ‹è¯•:")
    print(f"   Input shape: [{batch_size}, {seq_len}]")
    print(f"   Output shape: {outputs.shape}")
    print(f"   Sample outputs: {outputs[:3]}")
