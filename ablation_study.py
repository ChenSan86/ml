"""
æ¶ˆèå®éªŒä¸»è„šæœ¬ï¼šç³»ç»ŸåŒ–è¯„ä¼°æ¯ä¸ªæ”¹è¿›æ–¹å‘çš„æ•ˆæœ
"""
import os
import json
import time
import torch
import torch.nn as nn
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from tqdm import tqdm

from config import config
from dataset import prepare_data, get_dataloaders
from train import set_seed, compute_metrics, train_epoch, evaluate

# å¯¼å…¥ä¸åŒçš„æ¨¡å‹
from model import create_model as create_baseline_model
from model_attention import create_attention_model
from model_enhanced import create_enhanced_model


class AblationStudy:
    """æ¶ˆèå®éªŒç®¡ç†å™¨"""
    
    def __init__(self, output_dir='ablation_results'):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        self.results = []
        self.experiment_log = []
        
        # è®¾ç½®éšæœºç§å­
        set_seed(config.SEED)
        
        # å‡†å¤‡æ•°æ®ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰
        print("\n" + "="*80)
        print("ğŸ“‚ å‡†å¤‡æ•°æ®...")
        print("="*80)
        self.train_df, self.val_df = prepare_data()
        self.train_loader, self.val_loader = get_dataloaders(
            self.train_df, self.val_df,
            batch_size=config.BATCH_SIZE,
            num_workers=config.NUM_WORKERS
        )
        
        print(f"\nâœ… æ•°æ®å‡†å¤‡å®Œæˆ")
        print(f"   è®­ç»ƒé›†: {len(self.train_df):,} æ¡")
        print(f"   éªŒè¯é›†: {len(self.val_df):,} æ¡")
    
    def train_single_experiment(self, model, model_name, epochs=5):
        """
        è®­ç»ƒå•ä¸ªå®éªŒ
        
        Args:
            model: æ¨¡å‹å®ä¾‹
            model_name: æ¨¡å‹åç§°
            epochs: è®­ç»ƒè½®æ•°
        """
        print("\n" + "="*80)
        print(f"ğŸš€ å®éªŒ: {model_name}")
        print("="*80)
        
        model = model.to(config.DEVICE)
        
        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.BCEWithLogitsLoss()
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config.LEARNING_RATE,
            weight_decay=config.WEIGHT_DECAY
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=epochs, eta_min=1e-6
        )
        
        # è®°å½•
        history = {
            'train_loss': [],
            'train_auc': [],
            'val_loss': [],
            'val_auc': []
        }
        
        best_auc = 0.0
        start_time = time.time()
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(1, epochs + 1):
            print(f"\n--- Epoch {epoch}/{epochs} ---")
            
            # è®­ç»ƒ
            train_loss, train_auc = train_epoch(
                model, self.train_loader, criterion, optimizer, config.DEVICE
            )
            
            # éªŒè¯
            val_loss, val_metrics = evaluate(
                model, self.val_loader, criterion, config.DEVICE
            )
            
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()
            
            # è®°å½•
            history['train_loss'].append(train_loss)
            history['train_auc'].append(train_auc)
            history['val_loss'].append(val_loss)
            history['val_auc'].append(val_metrics['auc'])
            
            # æ›´æ–°æœ€ä½³
            if val_metrics['auc'] > best_auc:
                best_auc = val_metrics['auc']
            
            print(f"Train Loss: {train_loss:.4f}, Train AUC: {train_auc:.4f}")
            print(f"Val Loss: {val_loss:.4f}, Val AUC: {val_metrics['auc']:.4f}")
        
        total_time = time.time() - start_time
        
        # ä¿å­˜ç»“æœ
        result = {
            'model_name': model_name,
            'best_val_auc': best_auc,
            'final_val_auc': val_metrics['auc'],
            'final_accuracy': val_metrics['accuracy'],
            'final_precision': val_metrics['precision'],
            'final_recall': val_metrics['recall'],
            'final_f1': val_metrics['f1'],
            'training_time': total_time,
            'epochs': epochs,
            'history': history
        }
        
        self.results.append(result)
        
        print(f"\nâœ… {model_name} å®Œæˆ!")
        print(f"   Best AUC: {best_auc:.4f}")
        print(f"   Training Time: {total_time:.1f}s")
        
        # æ¸…ç†å†…å­˜
        del model, optimizer, scheduler
        torch.cuda.empty_cache()
        
        return result
    
    def run_baseline_experiment(self, epochs=5):
        """å®éªŒ1: åŸºçº¿æ¨¡å‹ï¼ˆBiLSTMåŒå¡”ï¼‰"""
        print("\n" + "="*80)
        print("ğŸ“Š å®éªŒ 1/10: åŸºçº¿æ¨¡å‹ï¼ˆBiLSTMåŒå¡”ï¼‰")
        print("="*80)
        
        model = create_baseline_model()
        result = self.train_single_experiment(model, "1_Baseline_BiLSTM", epochs)
        
        self.log_experiment(
            experiment_id=1,
            name="Baseline BiLSTM",
            description="åŒå¡”BiLSTM + äº¤äº’å±‚",
            improvements=[]
        )
        
        return result
    
    def run_attention_experiment(self, epochs=5):
        """å®éªŒ2: åŸºçº¿ + æ³¨æ„åŠ›æœºåˆ¶"""
        print("\n" + "="*80)
        print("ğŸ“Š å®éªŒ 2/10: åŸºçº¿ + æ³¨æ„åŠ›æœºåˆ¶")
        print("="*80)
        
        model = create_attention_model()
        result = self.train_single_experiment(model, "2_Baseline+Attention", epochs)
        
        self.log_experiment(
            experiment_id=2,
            name="Baseline + Attention",
            description="åœ¨BiLSTMåŸºç¡€ä¸Šæ·»åŠ è‡ªæ³¨æ„åŠ›æœºåˆ¶",
            improvements=["Attention Mechanism"]
        )
        
        return result
    
    def run_feature_engineering_experiment(self, epochs=5):
        """å®éªŒ3: åŸºçº¿ + ç‰¹å¾å·¥ç¨‹"""
        print("\n" + "="*80)
        print("ğŸ“Š å®éªŒ 3/10: åŸºçº¿ + ç‰¹å¾å·¥ç¨‹ï¼ˆ19ä¸ªæ‰‹å·¥ç‰¹å¾ï¼‰")
        print("="*80)
        
        model = create_enhanced_model()
        result = self.train_single_experiment(model, "3_Baseline+Features", epochs)
        
        self.log_experiment(
            experiment_id=3,
            name="Baseline + Feature Engineering",
            description="æ·»åŠ 19ä¸ªæ‰‹å·¥ç‰¹å¾ï¼ˆé•¿åº¦ã€é‡å ã€ç›¸ä¼¼åº¦ç­‰ï¼‰",
            improvements=["19 Handcrafted Features"]
        )
        
        return result
    
    def run_focal_loss_experiment(self, epochs=5):
        """å®éªŒ4: åŸºçº¿ + Focal Loss"""
        print("\n" + "="*80)
        print("ğŸ“Š å®éªŒ 4/10: åŸºçº¿ + Focal Lossï¼ˆéš¾æ ·æœ¬æŒ–æ˜ï¼‰")
        print("="*80)
        
        from losses import FocalLoss
        
        model = create_baseline_model()
        model = model.to(config.DEVICE)
        
        # ä½¿ç”¨Focal Loss
        criterion = FocalLoss(alpha=0.25, gamma=2.0)
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config.LEARNING_RATE,
            weight_decay=config.WEIGHT_DECAY
        )
        
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=epochs, eta_min=1e-6
        )
        
        best_auc = 0.0
        start_time = time.time()
        
        for epoch in range(1, epochs + 1):
            print(f"\n--- Epoch {epoch}/{epochs} ---")
            
            # è®­ç»ƒ
            model.train()
            total_loss = 0
            all_labels, all_probs = [], []
            
            for batch in tqdm(self.train_loader, desc='Training'):
                query1 = batch['query1'].to(config.DEVICE)
                query2 = batch['query2'].to(config.DEVICE)
                labels = batch['label'].to(config.DEVICE)
                
                optimizer.zero_grad()
                logits = model(query1, query2)
                loss = criterion(logits, labels)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                
                total_loss += loss.item()
                probs = torch.sigmoid(logits).detach().cpu().numpy()
                all_labels.extend(labels.cpu().numpy())
                all_probs.extend(probs)
            
            from sklearn.metrics import roc_auc_score
            train_loss = total_loss / len(self.train_loader)
            train_auc = roc_auc_score(all_labels, all_probs)
            
            # éªŒè¯ï¼ˆä½¿ç”¨BCEè¯„ä¼°ï¼‰
            val_loss, val_metrics = evaluate(
                model, self.val_loader, nn.BCEWithLogitsLoss(), config.DEVICE
            )
            
            scheduler.step()
            
            if val_metrics['auc'] > best_auc:
                best_auc = val_metrics['auc']
            
            print(f"Train Loss: {train_loss:.4f}, Train AUC: {train_auc:.4f}")
            print(f"Val Loss: {val_loss:.4f}, Val AUC: {val_metrics['auc']:.4f}")
        
        total_time = time.time() - start_time
        
        result = {
            'model_name': "4_Baseline+FocalLoss",
            'best_val_auc': best_auc,
            'final_val_auc': val_metrics['auc'],
            'final_accuracy': val_metrics['accuracy'],
            'final_precision': val_metrics['precision'],
            'final_recall': val_metrics['recall'],
            'final_f1': val_metrics['f1'],
            'training_time': total_time,
            'epochs': epochs
        }
        
        self.results.append(result)
        self.log_experiment(
            experiment_id=4,
            name="Baseline + Focal Loss",
            description="ä½¿ç”¨Focal Losså¤„ç†éš¾æ ·æœ¬",
            improvements=["Focal Loss"]
        )
        
        del model, optimizer, scheduler
        torch.cuda.empty_cache()
        
        return result
    
    def run_all_experiments(self, epochs_per_experiment=5):
        """è¿è¡Œæ‰€æœ‰æ¶ˆèå®éªŒ"""
        print("\n" + "="*80)
        print("ğŸ¯ å¼€å§‹å®Œæ•´æ¶ˆèå®éªŒ")
        print("="*80)
        print(f"\né…ç½®:")
        print(f"   æ¯ä¸ªå®éªŒè®­ç»ƒè½®æ•°: {epochs_per_experiment}")
        print(f"   é¢„è®¡æ€»æ—¶é—´: ~{epochs_per_experiment * 10 * 10}åˆ†é’Ÿ")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        
        start_time = time.time()
        
        # å®éªŒ1: åŸºçº¿
        self.run_baseline_experiment(epochs_per_experiment)
        
        # å®éªŒ2: + æ³¨æ„åŠ›
        self.run_attention_experiment(epochs_per_experiment)
        
        # å®éªŒ3: + ç‰¹å¾å·¥ç¨‹
        self.run_feature_engineering_experiment(epochs_per_experiment)
        
        # å®éªŒ4: + Focal Loss
        self.run_focal_loss_experiment(epochs_per_experiment)
        
        # æ›´å¤šå®éªŒå¯ä»¥ç»§ç»­æ·»åŠ ...
        
        total_time = time.time() - start_time
        
        print("\n" + "="*80)
        print("ğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆ!")
        print("="*80)
        print(f"æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report()
        
        return self.results
    
    def log_experiment(self, experiment_id, name, description, improvements):
        """è®°å½•å®éªŒä¿¡æ¯"""
        self.experiment_log.append({
            'id': experiment_id,
            'name': name,
            'description': description,
            'improvements': improvements,
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        })
    
    def generate_report(self):
        """ç”Ÿæˆå®éªŒæŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ“Š ç”Ÿæˆå®éªŒæŠ¥å‘Š")
        print("="*80)
        
        # ä¿å­˜ç»“æœä¸ºJSON
        report_path = os.path.join(self.output_dir, 'results.json')
        with open(report_path, 'w') as f:
            json.dump({
                'results': self.results,
                'experiments': self.experiment_log,
                'config': {
                    'seed': config.SEED,
                    'batch_size': config.BATCH_SIZE,
                    'learning_rate': config.LEARNING_RATE
                }
            }, f, indent=2)
        
        print(f"âœ… JSONç»“æœå·²ä¿å­˜: {report_path}")
        
        # ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
        self._generate_comparison_table()
        
        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self._generate_visualizations()
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self._generate_markdown_report()
    
    def _generate_comparison_table(self):
        """ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼"""
        df = pd.DataFrame(self.results)
        
        # é€‰æ‹©å…³é”®åˆ—
        comparison_df = df[[
            'model_name', 'best_val_auc', 'final_accuracy', 
            'final_f1', 'training_time'
        ]].copy()
        
        # è®¡ç®—ç›¸å¯¹æå‡
        baseline_auc = comparison_df.iloc[0]['best_val_auc']
        comparison_df['auc_improvement'] = \
            (comparison_df['best_val_auc'] - baseline_auc) * 100
        
        # ä¿å­˜ä¸ºCSV
        csv_path = os.path.join(self.output_dir, 'comparison.csv')
        comparison_df.to_csv(csv_path, index=False)
        
        print(f"âœ… å¯¹æ¯”è¡¨æ ¼å·²ä¿å­˜: {csv_path}")
        
        # æ‰“å°è¡¨æ ¼
        print("\n" + "="*80)
        print("ğŸ“ˆ å®éªŒç»“æœå¯¹æ¯”")
        print("="*80)
        print(comparison_df.to_string(index=False))
    
    def _generate_visualizations(self):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        df = pd.DataFrame(self.results)
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. AUCå¯¹æ¯”
        ax1 = axes[0, 0]
        models = [r['model_name'] for r in self.results]
        aucs = [r['best_val_auc'] for r in self.results]
        colors = plt.cm.viridis(range(len(models)))
        
        bars = ax1.bar(range(len(models)), aucs, color=colors)
        ax1.set_xlabel('Model', fontsize=12, fontweight='bold')
        ax1.set_ylabel('Best Validation AUC', fontsize=12, fontweight='bold')
        ax1.set_title('AUC Comparison Across Models', fontsize=14, fontweight='bold')
        ax1.set_xticks(range(len(models)))
        ax1.set_xticklabels([m.replace('_', '\n') for m in models], 
                           rotation=45, ha='right', fontsize=9)
        ax1.grid(axis='y', alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, auc) in enumerate(zip(bars, aucs)):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height,
                    f'{auc:.4f}',
                    ha='center', va='bottom', fontsize=10, fontweight='bold')
        
        # 2. ç›¸å¯¹æå‡
        ax2 = axes[0, 1]
        baseline_auc = aucs[0]
        improvements = [(auc - baseline_auc) * 100 for auc in aucs]
        
        bars = ax2.bar(range(len(models)), improvements, color=colors)
        ax2.set_xlabel('Model', fontsize=12, fontweight='bold')
        ax2.set_ylabel('AUC Improvement (%)', fontsize=12, fontweight='bold')
        ax2.set_title('Relative Improvement over Baseline', fontsize=14, fontweight='bold')
        ax2.set_xticks(range(len(models)))
        ax2.set_xticklabels([m.replace('_', '\n') for m in models], 
                           rotation=45, ha='right', fontsize=9)
        ax2.axhline(y=0, color='r', linestyle='--', linewidth=2)
        ax2.grid(axis='y', alpha=0.3)
        
        for bar, imp in zip(bars, improvements):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height,
                    f'{imp:+.2f}%',
                    ha='center', va='bottom' if imp >= 0 else 'top',
                    fontsize=10, fontweight='bold')
        
        # 3. è®­ç»ƒæ—¶é—´å¯¹æ¯”
        ax3 = axes[1, 0]
        times = [r['training_time'] / 60 for r in self.results]  # è½¬ä¸ºåˆ†é’Ÿ
        
        bars = ax3.bar(range(len(models)), times, color=colors)
        ax3.set_xlabel('Model', fontsize=12, fontweight='bold')
        ax3.set_ylabel('Training Time (minutes)', fontsize=12, fontweight='bold')
        ax3.set_title('Training Time Comparison', fontsize=14, fontweight='bold')
        ax3.set_xticks(range(len(models)))
        ax3.set_xticklabels([m.replace('_', '\n') for m in models], 
                           rotation=45, ha='right', fontsize=9)
        ax3.grid(axis='y', alpha=0.3)
        
        for bar, t in zip(bars, times):
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height,
                    f'{t:.1f}m',
                    ha='center', va='bottom', fontsize=10)
        
        # 4. F1åˆ†æ•°å¯¹æ¯”
        ax4 = axes[1, 1]
        f1_scores = [r['final_f1'] for r in self.results]
        
        bars = ax4.bar(range(len(models)), f1_scores, color=colors)
        ax4.set_xlabel('Model', fontsize=12, fontweight='bold')
        ax4.set_ylabel('F1 Score', fontsize=12, fontweight='bold')
        ax4.set_title('F1 Score Comparison', fontsize=14, fontweight='bold')
        ax4.set_xticks(range(len(models)))
        ax4.set_xticklabels([m.replace('_', '\n') for m in models], 
                           rotation=45, ha='right', fontsize=9)
        ax4.grid(axis='y', alpha=0.3)
        
        for bar, f1 in zip(bars, f1_scores):
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2., height,
                    f'{f1:.4f}',
                    ha='center', va='bottom', fontsize=10)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        viz_path = os.path.join(self.output_dir, 'ablation_comparison.png')
        plt.savefig(viz_path, dpi=300, bbox_inches='tight')
        print(f"âœ… å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: {viz_path}")
        
        plt.close()
    
    def _generate_markdown_report(self):
        """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        report_lines = []
        
        # æ ‡é¢˜
        report_lines.append("# æ¶ˆèå®éªŒæŠ¥å‘Š (Ablation Study Report)")
        report_lines.append("")
        report_lines.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append("")
        report_lines.append("---")
        report_lines.append("")
        
        # å®éªŒé…ç½®
        report_lines.append("## ğŸ“‹ å®éªŒé…ç½®")
        report_lines.append("")
        report_lines.append(f"- **éšæœºç§å­**: {config.SEED}")
        report_lines.append(f"- **æ‰¹æ¬¡å¤§å°**: {config.BATCH_SIZE}")
        report_lines.append(f"- **å­¦ä¹ ç‡**: {config.LEARNING_RATE}")
        report_lines.append(f"- **æ•°æ®è§„æ¨¡**: è®­ç»ƒé›† {len(self.train_df):,} / éªŒè¯é›† {len(self.val_df):,}")
        report_lines.append("")
        report_lines.append("---")
        report_lines.append("")
        
        # å®éªŒç»“æœæ€»è¡¨
        report_lines.append("## ğŸ“Š å®éªŒç»“æœæ€»è§ˆ")
        report_lines.append("")
        
        # åˆ›å»ºè¡¨æ ¼
        report_lines.append("| å®éªŒID | æ¨¡å‹åç§° | AUC | å‡†ç¡®ç‡ | F1 | è®­ç»ƒæ—¶é—´ | ç›¸å¯¹æå‡ |")
        report_lines.append("|--------|---------|-----|--------|----|---------|---------|")
        
        baseline_auc = self.results[0]['best_val_auc']
        
        for i, result in enumerate(self.results, 1):
            improvement = (result['best_val_auc'] - baseline_auc) * 100
            report_lines.append(
                f"| {i} | {result['model_name']} | "
                f"{result['best_val_auc']:.4f} | "
                f"{result['final_accuracy']:.4f} | "
                f"{result['final_f1']:.4f} | "
                f"{result['training_time']/60:.1f}min | "
                f"{improvement:+.2f}% |"
            )
        
        report_lines.append("")
        report_lines.append("---")
        report_lines.append("")
        
        # å…³é”®å‘ç°
        report_lines.append("## ğŸ” å…³é”®å‘ç°")
        report_lines.append("")
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_result = max(self.results, key=lambda x: x['best_val_auc'])
        report_lines.append(f"### æœ€ä½³æ¨¡å‹")
        report_lines.append(f"- **åç§°**: {best_result['model_name']}")
        report_lines.append(f"- **AUC**: {best_result['best_val_auc']:.4f}")
        report_lines.append(f"- **ç›¸å¯¹åŸºçº¿æå‡**: {(best_result['best_val_auc'] - baseline_auc)*100:+.2f}%")
        report_lines.append("")
        
        # å„æ”¹è¿›çš„è´¡çŒ®
        report_lines.append("### å„æ”¹è¿›æ–¹å‘è´¡çŒ®")
        for i, result in enumerate(self.results):
            if i == 0:
                continue  # è·³è¿‡åŸºçº¿
            improvement = (result['best_val_auc'] - baseline_auc) * 100
            exp_info = self.experiment_log[i]
            report_lines.append(f"- **{exp_info['name']}**: {improvement:+.2f}%")
        
        report_lines.append("")
        report_lines.append("---")
        report_lines.append("")
        
        # ç»“è®º
        report_lines.append("## ğŸ’¡ ç»“è®º")
        report_lines.append("")
        report_lines.append(f"1. åŸºçº¿æ¨¡å‹AUCä¸º **{baseline_auc:.4f}**")
        report_lines.append(f"2. æœ€ä½³æ¨¡å‹AUCè¾¾åˆ° **{best_result['best_val_auc']:.4f}**")
        report_lines.append(f"3. æ€»ä½“æå‡ **{(best_result['best_val_auc'] - baseline_auc)*100:.2f}%**")
        report_lines.append("")
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = os.path.join(self.output_dir, 'REPORT.md')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        print(f"âœ… MarkdownæŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        # æ‰“å°åˆ°æ§åˆ¶å°
        print("\n" + "="*80)
        print('\n'.join(report_lines))


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ¶ˆèå®éªŒ')
    parser.add_argument('--epochs', type=int, default=5, 
                       help='æ¯ä¸ªå®éªŒçš„è®­ç»ƒè½®æ•° (é»˜è®¤: 5)')
    parser.add_argument('--output_dir', type=str, default='ablation_results',
                       help='è¾“å‡ºç›®å½• (é»˜è®¤: ablation_results)')
    
    args = parser.parse_args()
    
    print("\n" + "="*80)
    print("ğŸ¯ æ¶ˆèå®éªŒç³»ç»Ÿ")
    print("="*80)
    print(f"\né…ç½®:")
    print(f"   æ¯ä¸ªå®éªŒè½®æ•°: {args.epochs}")
    print(f"   è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"   è®¾å¤‡: {config.DEVICE}")
    
    # åˆ›å»ºå®éªŒç®¡ç†å™¨
    study = AblationStudy(output_dir=args.output_dir)
    
    # è¿è¡Œæ‰€æœ‰å®éªŒ
    results = study.run_all_experiments(epochs_per_experiment=args.epochs)
    
    print("\n" + "="*80)
    print("âœ… å®éªŒå®Œæˆï¼è¯·æŸ¥çœ‹æŠ¥å‘Š:")
    print(f"   - JSON: {args.output_dir}/results.json")
    print(f"   - CSV: {args.output_dir}/comparison.csv")
    print(f"   - å›¾è¡¨: {args.output_dir}/ablation_comparison.png")
    print(f"   - æŠ¥å‘Š: {args.output_dir}/REPORT.md")
    print("="*80)


if __name__ == '__main__':
    main()
