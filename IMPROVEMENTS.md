# ğŸ“ˆ æ¨¡å‹æ”¹è¿›æ–¹æ¡ˆæ±‡æ€»

## ğŸ¯ å½“å‰çŠ¶æ€

**å·²å®ç°**ï¼š
- âœ… BiLSTMåŒå¡”æ¨¡å‹ï¼ˆAUC 0.9718ï¼‰
- âœ… ç‰¹å¾å·¥ç¨‹ï¼ˆ19ä¸ªæ‰‹å·¥ç‰¹å¾ï¼‰
- âœ… KæŠ˜äº¤å‰éªŒè¯
- âœ… æ•°æ®å¢å¼ºï¼ˆé€šè¿‡dropoutï¼‰

**æ€§èƒ½**ï¼š
- éªŒè¯é›†AUCï¼š0.9718
- å‡†ç¡®ç‡ï¼š0.9252
- F1åˆ†æ•°ï¼š0.9029

---

## ğŸš€ æ”¹è¿›æ–¹å‘æ€»è§ˆ

| æ”¹è¿›æ–¹å‘ | éš¾åº¦ | é¢„æœŸæå‡ | å®ç°æ—¶é—´ |
|---------|------|---------|---------|
| 1. æ³¨æ„åŠ›æœºåˆ¶ | â­â­ | +0.5~1% | 2å°æ—¶ |
| 2. å¯¹æ¯”å­¦ä¹  | â­â­â­ | +1~2% | 4å°æ—¶ |
| 3. æ•°æ®å¢å¼º | â­â­ | +0.5~1% | 3å°æ—¶ |
| 4. Transformeræ¶æ„ | â­â­â­â­ | +2~3% | 6å°æ—¶ |
| 5. æ¨¡å‹é›†æˆ | â­ | +0.5~1% | 1å°æ—¶ |
| 6. éš¾æ ·æœ¬æŒ–æ˜ | â­â­ | +0.5~1% | 2å°æ—¶ |
| 7. æŸå¤±å‡½æ•°ä¼˜åŒ– | â­â­ | +0.3~0.5% | 1å°æ—¶ |
| 8. é¢„è®­ç»ƒè¯å‘é‡ | â­â­â­ | +1~2% | 4å°æ—¶ |

---

## 1ï¸âƒ£ æ³¨æ„åŠ›æœºåˆ¶ â­â­ (æ¨è)

### ä¸ºä»€ä¹ˆéœ€è¦ï¼Ÿ
å½“å‰æ¨¡å‹ä½¿ç”¨BiLSTMçš„æœ€åçŠ¶æ€ï¼Œå¯èƒ½ä¸¢å¤±é‡è¦ä¿¡æ¯ã€‚æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥å…³æ³¨å…³é”®è¯ã€‚

### å®ç°æ–¹æ¡ˆ

```python
class AttentionLayer(nn.Module):
    """è‡ªæ³¨æ„åŠ›å±‚"""
    def __init__(self, hidden_dim):
        super().__init__()
        self.attention = nn.Linear(hidden_dim, 1)
    
    def forward(self, lstm_output):
        # lstm_output: [batch, seq_len, hidden_dim]
        weights = torch.softmax(self.attention(lstm_output), dim=1)
        attended = torch.sum(weights * lstm_output, dim=1)
        return attended

# åœ¨encoderä¸­ä½¿ç”¨
repr = self.attention(lstm_out)  # æ›¿ä»£å–æœ€åæ—¶åˆ»
```

### é¢„æœŸæ•ˆæœ
- AUC: 0.9718 â†’ **0.9730+**
- æ›´å¥½åœ°æ•æ‰å…³é”®è¯ä¿¡æ¯

---

## 2ï¸âƒ£ å¯¹æ¯”å­¦ä¹  (Contrastive Learning) â­â­â­

### åŸç†
è®©ç›¸ä¼¼çš„æ ·æœ¬åœ¨embeddingç©ºé—´é è¿‘ï¼Œä¸ç›¸ä¼¼çš„æ ·æœ¬è¿œç¦»ã€‚

### å®ç°æ–¹æ¡ˆ

```python
class ContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0):
        super().__init__()
        self.margin = margin
    
    def forward(self, repr1, repr2, labels):
        # æ¬§æ°è·ç¦»
        distance = F.pairwise_distance(repr1, repr2)
        
        # å¯¹æ¯”æŸå¤±
        loss = labels * distance.pow(2) + \
               (1 - labels) * F.relu(self.margin - distance).pow(2)
        return loss.mean()

# ç»“åˆBCEå’Œå¯¹æ¯”æŸå¤±
total_loss = bce_loss + 0.2 * contrastive_loss
```

### é¢„æœŸæ•ˆæœ
- AUC: 0.9718 â†’ **0.9740+**
- æ›´å¥½çš„è¯­ä¹‰è¡¨ç¤ºç©ºé—´

---

## 3ï¸âƒ£ æ•°æ®å¢å¼º â­â­

### å½“å‰é—®é¢˜
è®­ç»ƒæ•°æ®å›ºå®šï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆã€‚

### å¢å¼ºç­–ç•¥

#### æ–¹æ¡ˆAï¼šè¯çº§å¢å¼º
```python
def word_dropout(ids, p=0.1):
    """éšæœºåˆ é™¤è¯"""
    mask = torch.rand(len(ids)) > p
    return ids[mask]

def word_shuffle(ids, k=3):
    """å±€éƒ¨æ‰“ä¹±"""
    # åœ¨kèŒƒå›´å†…éšæœºäº¤æ¢
    for i in range(len(ids) - k):
        if random.random() < 0.5:
            j = random.randint(i, min(i+k, len(ids)-1))
            ids[i], ids[j] = ids[j], ids[i]
    return ids
```

#### æ–¹æ¡ˆBï¼šå›è¯‘ï¼ˆéœ€è¦å¤–éƒ¨æ¨¡å‹ï¼‰
```python
# æ­£æ ·æœ¬å¯¹ï¼šäº’æ¢query1å’Œquery2
if label == 1:
    augmented_samples.append((query2, query1, 1))
```

#### æ–¹æ¡ˆCï¼šè´Ÿæ ·æœ¬ç”Ÿæˆ
```python
# éšæœºé…å¯¹ç”Ÿæˆå›°éš¾è´Ÿæ ·æœ¬
def create_hard_negatives(df):
    pos_samples = df[df['label'] == 1]
    # éšæœºæ‰“ä¹±query2
    shuffled = pos_samples.sample(frac=1)
    hard_negs = pd.DataFrame({
        'query1': pos_samples['query1'].values,
        'query2': shuffled['query2'].values,
        'label': 0
    })
    return hard_negs
```

### é¢„æœŸæ•ˆæœ
- AUC: 0.9718 â†’ **0.9735+**
- æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›

---

## 4ï¸âƒ£ Transformeræ¶æ„ â­â­â­â­

### ä¸ºä»€ä¹ˆæ›´å¥½ï¼Ÿ
- å¹¶è¡Œè®¡ç®—ï¼Œæ›´å¿«
- é•¿è·ç¦»ä¾èµ–å»ºæ¨¡
- é¢„è®­ç»ƒæ¨¡å‹å¯ç”¨

### å®ç°æ–¹æ¡ˆ

```python
class TransformerEncoder(nn.Module):
    def __init__(self, vocab_size, embed_dim, num_heads=8, num_layers=4):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.pos_encoding = PositionalEncoding(embed_dim)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=num_heads,
            dim_feedforward=2048,
            dropout=0.1
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        
    def forward(self, x):
        embedded = self.embedding(x)
        embedded = self.pos_encoding(embedded)
        output = self.transformer(embedded)
        return output.mean(dim=1)  # å¹³å‡æ± åŒ–
```

### é¢„æœŸæ•ˆæœ
- AUC: 0.9718 â†’ **0.9750+**
- è®­ç»ƒé€Ÿåº¦æå‡2-3å€

---

## 5ï¸âƒ£ æ¨¡å‹é›†æˆ â­ (æœ€ç®€å•ï¼Œæ•ˆæœå¥½)

### æ–¹æ¡ˆAï¼šå¤šæ¨¡å‹æŠ•ç¥¨
```python
# è®­ç»ƒå¤šä¸ªä¸åŒçš„æ¨¡å‹
models = [
    BiLSTM_model,
    Transformer_model,
    Enhanced_model
]

# é¢„æµ‹æ—¶å¹³å‡
probs = np.mean([model.predict(x) for model in models], axis=0)
```

### æ–¹æ¡ˆBï¼šStacking
```python
# ç¬¬ä¸€å±‚ï¼šå¤šä¸ªåŸºæ¨¡å‹
base_models = [model1, model2, model3]
base_preds = [m.predict(X_val) for m in base_models]

# ç¬¬äºŒå±‚ï¼šå…ƒå­¦ä¹ å™¨
meta_model = LogisticRegression()
meta_model.fit(np.column_stack(base_preds), y_val)
```

### é¢„æœŸæ•ˆæœ
- AUC: 0.9718 â†’ **0.9730+**
- ç¨³å®šæ€§æå‡

---

## 6ï¸âƒ£ éš¾æ ·æœ¬æŒ–æ˜ (Hard Negative Mining) â­â­

### åŸç†
é‡ç‚¹å­¦ä¹ æ¨¡å‹å®¹æ˜“å‡ºé”™çš„æ ·æœ¬ã€‚

### å®ç°æ–¹æ¡ˆ

```python
class FocalLoss(nn.Module):
    """Focal Loss: å¯¹éš¾æ ·æœ¬èµ‹äºˆæ›´é«˜æƒé‡"""
    def __init__(self, alpha=0.25, gamma=2.0):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
    
    def forward(self, logits, targets):
        bce_loss = F.binary_cross_entropy_with_logits(
            logits, targets, reduction='none'
        )
        probs = torch.sigmoid(logits)
        pt = targets * probs + (1 - targets) * (1 - probs)
        focal_weight = (1 - pt) ** self.gamma
        loss = self.alpha * focal_weight * bce_loss
        return loss.mean()

# ä½¿ç”¨Focal Lossæ›¿ä»£BCE
criterion = FocalLoss()
```

### é¢„æœŸæ•ˆæœ
- AUC: 0.9718 â†’ **0.9728+**
- å‡å°‘è¾¹ç•Œæ ·æœ¬é”™è¯¯

---

## 7ï¸âƒ£ æŸå¤±å‡½æ•°ä¼˜åŒ– â­â­

### æ–¹æ¡ˆAï¼šå¤šä»»åŠ¡å­¦ä¹ 
```python
# åŒæ—¶é¢„æµ‹å¤šä¸ªç›®æ ‡
loss = bce_loss + 0.3 * ranking_loss + 0.2 * triplet_loss
```

### æ–¹æ¡ˆBï¼šæ ‡ç­¾å¹³æ»‘
```python
# é¿å…è¿‡åº¦è‡ªä¿¡
def label_smoothing(labels, epsilon=0.1):
    return labels * (1 - epsilon) + 0.5 * epsilon
```

### æ–¹æ¡ˆCï¼šç±»åˆ«æƒé‡
```python
# å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
pos_weight = (negative_count / positive_count)
criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
```

### é¢„æœŸæ•ˆæœ
- AUC: 0.9718 â†’ **0.9725+**

---

## 8ï¸âƒ£ é¢„è®­ç»ƒè¯å‘é‡ â­â­â­

### æ–¹æ¡ˆAï¼šWord2Vecé¢„è®­ç»ƒ
```python
# åœ¨å½“å‰æ•°æ®ä¸Šé¢„è®­ç»ƒ
from gensim.models import Word2Vec

# å‡†å¤‡è¯­æ–™
corpus = []
for text in all_texts:
    corpus.append([str(id) for id in text.split()])

# è®­ç»ƒWord2Vec
model = Word2Vec(corpus, vector_size=300, window=5, min_count=1)

# åˆå§‹åŒ–embedding
pretrained_weights = np.zeros((vocab_size, 300))
for word_id in range(vocab_size):
    if str(word_id) in model.wv:
        pretrained_weights[word_id] = model.wv[str(word_id)]

# åŠ è½½åˆ°æ¨¡å‹
model.embedding.weight.data.copy_(torch.from_numpy(pretrained_weights))
```

### æ–¹æ¡ˆBï¼šè‡ªç›‘ç£é¢„è®­ç»ƒ
```python
# Masked Language Model
def mlm_pretrain(texts):
    for text in texts:
        # éšæœºmask 15%çš„è¯
        masked_text, targets = mask_tokens(text)
        # é¢„æµ‹è¢«maskçš„è¯
        loss = criterion(model(masked_text), targets)
```

### é¢„æœŸæ•ˆæœ
- AUC: 0.9718 â†’ **0.9740+**
- æ›´å¥½çš„è¯è¡¨ç¤º

---

## 9ï¸âƒ£ è®­ç»ƒä¼˜åŒ–

### A. æ··åˆç²¾åº¦è®­ç»ƒ
```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    logits = model(query1, query2)
    loss = criterion(logits, labels)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```
**æ•ˆæœ**ï¼šè®­ç»ƒé€Ÿåº¦æå‡30-50%ï¼Œæ˜¾å­˜å ç”¨å‡åŠ

### B. æ¢¯åº¦ç´¯ç§¯
```python
accumulation_steps = 4

for i, batch in enumerate(train_loader):
    loss = loss / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```
**æ•ˆæœ**ï¼šç­‰æ•ˆæ›´å¤§batch sizeï¼Œæ›´ç¨³å®š

### C. å­¦ä¹ ç‡ç­–ç•¥
```python
# Warmup + Cosine
from transformers import get_cosine_schedule_with_warmup

scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps=warmup_steps,
    num_training_steps=total_steps
)
```

---

## ğŸ”Ÿ ç‰¹å¾å·¥ç¨‹å¢å¼º

### å½“å‰19ä¸ªç‰¹å¾åŸºç¡€ä¸Šï¼Œå¯ä»¥æ·»åŠ ï¼š

```python
# 1. TF-IDFç‰¹å¾
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(max_features=1000)
tfidf_features = tfidf.fit_transform(texts)

# 2. N-gramç‰¹å¾
def get_ngrams(text, n=2):
    ngrams = zip(*[text[i:] for i in range(n)])
    return [' '.join(gram) for gram in ngrams]

# 3. æ¨¡ç³ŠåŒ¹é…åˆ†æ•°
from fuzzywuzzy import fuzz

fuzzy_ratio = fuzz.ratio(query1, query2)
fuzzy_partial = fuzz.partial_ratio(query1, query2)

# 4. åºåˆ—æ¨¡å¼ç‰¹å¾
def longest_common_subsequence(s1, s2):
    # LCSé•¿åº¦
    pass

# 5. ä½ç½®ç‰¹å¾
first_common_pos = # ç¬¬ä¸€ä¸ªå…¬å…±è¯çš„ä½ç½®
last_common_pos = # æœ€åä¸€ä¸ªå…¬å…±è¯çš„ä½ç½®
```

**æ–°å¢ç‰¹å¾æ•°**ï¼š+10ä¸ª  
**é¢„æœŸæå‡**ï¼š+0.3~0.5%

---

## ğŸ¨ å¯è§†åŒ–ä¸åˆ†æ

### A. æ³¨æ„åŠ›å¯è§†åŒ–
```python
import seaborn as sns

def visualize_attention(attention_weights, tokens):
    plt.figure(figsize=(10, 8))
    sns.heatmap(attention_weights, xticklabels=tokens, 
                yticklabels=tokens, cmap='YlOrRd')
    plt.show()
```

### B. é”™è¯¯æ¡ˆä¾‹åˆ†æ
```python
def analyze_errors(model, val_loader):
    errors = []
    for batch in val_loader:
        preds = model(batch)
        wrong_indices = (preds != batch['labels'])
        errors.extend(batch[wrong_indices])
    
    # åˆ†æé”™è¯¯æ¨¡å¼
    print("é”™è¯¯æ ·æœ¬ç‰¹å¾åˆ†æ:")
    print(f"å¹³å‡é•¿åº¦: {np.mean([len(e) for e in errors])}")
    print(f"å¹³å‡é‡å åº¦: ...")
```

### C. ç‰¹å¾é‡è¦æ€§
```python
from sklearn.ensemble import RandomForestClassifier

# ç”¨éšæœºæ£®æ—è¯„ä¼°ç‰¹å¾é‡è¦æ€§
rf = RandomForestClassifier()
rf.fit(handcrafted_features, labels)

importances = pd.DataFrame({
    'feature': feature_names,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)
```

---

## ğŸ“Š å®æ–½ä¼˜å…ˆçº§å»ºè®®

### ğŸ”¥ é«˜ä¼˜å…ˆçº§ï¼ˆå¿«é€Ÿè§æ•ˆï¼‰
1. **æ¨¡å‹é›†æˆ** (1å°æ—¶) â†’ +0.5~1%
2. **æ³¨æ„åŠ›æœºåˆ¶** (2å°æ—¶) â†’ +0.5~1%
3. **Focal Loss** (1å°æ—¶) â†’ +0.3~0.5%

### â­ ä¸­ä¼˜å…ˆçº§ï¼ˆä¸­ç­‰æŠ•å…¥ï¼‰
4. **æ•°æ®å¢å¼º** (3å°æ—¶) â†’ +0.5~1%
5. **å¯¹æ¯”å­¦ä¹ ** (4å°æ—¶) â†’ +1~2%
6. **éš¾æ ·æœ¬æŒ–æ˜** (2å°æ—¶) â†’ +0.5~1%

### ğŸ¯ é•¿æœŸä¼˜åŒ–ï¼ˆå¤§å¹…æ”¹è¿›ï¼‰
7. **Transformeræ¶æ„** (6å°æ—¶) â†’ +2~3%
8. **é¢„è®­ç»ƒè¯å‘é‡** (4å°æ—¶) â†’ +1~2%

---

## ğŸš€ å¿«é€Ÿå®æ–½æ–¹æ¡ˆ

### æ–¹æ¡ˆ1ï¼šä¿å®ˆå‹ï¼ˆ2å°æ—¶ï¼Œ+1~1.5%ï¼‰
```bash
1. æ·»åŠ æ³¨æ„åŠ›æœºåˆ¶
2. ä½¿ç”¨Focal Loss
3. KæŠ˜é›†æˆï¼ˆå·²æœ‰ä»£ç ï¼‰
```

### æ–¹æ¡ˆ2ï¼šè¿›å–å‹ï¼ˆ8å°æ—¶ï¼Œ+2~3%ï¼‰
```bash
1. å®ç°Transformeræ¨¡å‹
2. æ·»åŠ å¯¹æ¯”å­¦ä¹ 
3. æ•°æ®å¢å¼º
4. æ¨¡å‹é›†æˆ
```

### æ–¹æ¡ˆ3ï¼šå…¨é¢ä¼˜åŒ–ï¼ˆ20å°æ—¶ï¼Œ+3~5%ï¼‰
```bash
1. Transformer + æ³¨æ„åŠ›
2. é¢„è®­ç»ƒè¯å‘é‡
3. å¯¹æ¯”å­¦ä¹  + Focal Loss
4. å®Œæ•´æ•°æ®å¢å¼º
5. å¤šæ¨¡å‹Stackingé›†æˆ
6. ç‰¹å¾å·¥ç¨‹æ‰©å±•
```

---

## ğŸ“ æ€»ç»“

| å½“å‰æ€§èƒ½ | å¿«é€Ÿä¼˜åŒ–å | æ·±åº¦ä¼˜åŒ–å | ç†è®ºä¸Šé™ |
|---------|-----------|-----------|---------|
| **0.9718** | **0.9735** | **0.9760** | **0.9800+** |

**å»ºè®®è·¯çº¿**ï¼š
1. å…ˆå®ç°ç®€å•çš„é›†æˆå’Œæ³¨æ„åŠ›ï¼ˆ2å°æ—¶ï¼‰
2. å¦‚æœæ—¶é—´å……è£•ï¼Œå°è¯•Transformerï¼ˆ6å°æ—¶ï¼‰
3. æœ€åè€ƒè™‘é¢„è®­ç»ƒå’Œå¯¹æ¯”å­¦ä¹ ï¼ˆ4-8å°æ—¶ï¼‰

---

**éœ€è¦æˆ‘å¸®ä½ å®ç°å“ªä¸ªæ”¹è¿›æ–¹å‘å—ï¼Ÿ**
