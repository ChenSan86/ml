"""
æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°æ¨¡å—
"""
import os
import time
import torch
import torch.nn as nn
import numpy as np
from tqdm import tqdm
from sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_fscore_support
import matplotlib.pyplot as plt
import seaborn as sns

from config import config
from model import create_model
from dataset import prepare_data, get_dataloaders


def set_seed(seed):
    """è®¾ç½®éšæœºç§å­"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def compute_metrics(labels, preds, probs):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    auc = roc_auc_score(labels, probs)
    acc = accuracy_score(labels, preds)
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, preds, average='binary', zero_division=0
    )
    
    return {
        'auc': auc,
        'accuracy': acc,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }


def train_epoch(model, train_loader, criterion, optimizer, device):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0
    all_labels = []
    all_probs = []
    
    progress_bar = tqdm(train_loader, desc='Training')
    for batch in progress_bar:
        query1 = batch['query1'].to(device)
        query2 = batch['query2'].to(device)
        labels = batch['label'].to(device)
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        logits = model(query1, query2)
        
        # è®¡ç®—æŸå¤±
        loss = criterion(logits, labels)
        
        # åå‘ä¼ æ’­
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        # ç»Ÿè®¡
        total_loss += loss.item()
        probs = torch.sigmoid(logits).detach().cpu().numpy()
        all_labels.extend(labels.cpu().numpy())
        all_probs.extend(probs)
        
        # æ›´æ–°è¿›åº¦æ¡
        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})
    
    # è®¡ç®—å¹³å‡æŸå¤±å’ŒAUC
    avg_loss = total_loss / len(train_loader)
    auc = roc_auc_score(all_labels, all_probs)
    
    return avg_loss, auc


@torch.no_grad()
def evaluate(model, val_loader, criterion, device):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0
    all_labels = []
    all_probs = []
    all_preds = []
    
    progress_bar = tqdm(val_loader, desc='Evaluating')
    for batch in progress_bar:
        query1 = batch['query1'].to(device)
        query2 = batch['query2'].to(device)
        labels = batch['label'].to(device)
        
        # å‰å‘ä¼ æ’­
        logits = model(query1, query2)
        loss = criterion(logits, labels)
        
        # ç»Ÿè®¡
        total_loss += loss.item()
        probs = torch.sigmoid(logits).cpu().numpy()
        preds = (probs > 0.5).astype(int)
        
        all_labels.extend(labels.cpu().numpy())
        all_probs.extend(probs)
        all_preds.extend(preds)
    
    # è®¡ç®—æŒ‡æ ‡
    avg_loss = total_loss / len(val_loader)
    metrics = compute_metrics(all_labels, all_preds, all_probs)
    
    return avg_loss, metrics


def plot_training_history(history, save_path='training_history.png'):
    """ç»˜åˆ¶è®­ç»ƒå†å²æ›²çº¿"""
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Lossæ›²çº¿
    axes[0].plot(history['train_loss'], label='Train Loss', marker='o')
    axes[0].plot(history['val_loss'], label='Val Loss', marker='s')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].set_title('Training and Validation Loss')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # AUCæ›²çº¿
    axes[1].plot(history['train_auc'], label='Train AUC', marker='o')
    axes[1].plot(history['val_auc'], label='Val AUC', marker='s')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('AUC')
    axes[1].set_title('Training and Validation AUC')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"\nğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")


def train():
    """å®Œæ•´çš„è®­ç»ƒæµç¨‹"""
    print("=" * 70)
    print("ğŸš€ å¼€å§‹è®­ç»ƒæ–‡æœ¬åŒ¹é…æ¨¡å‹")
    print("=" * 70)
    
    # è®¾ç½®éšæœºç§å­
    set_seed(config.SEED)
    print(f"\nâš™ï¸  é…ç½®ä¿¡æ¯: {config}")
    
    # å‡†å¤‡æ•°æ®
    print("\nğŸ“‚ å‡†å¤‡æ•°æ®...")
    train_df, val_df = prepare_data()
    train_loader, val_loader = get_dataloaders(
        train_df, val_df, 
        batch_size=config.BATCH_SIZE,
        num_workers=config.NUM_WORKERS
    )
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ—ï¸  åˆ›å»ºæ¨¡å‹...")
    model = create_model()
    model = model.to(config.DEVICE)
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.BCEWithLogitsLoss()
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.LEARNING_RATE,
        weight_decay=config.WEIGHT_DECAY
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=config.EPOCHS, eta_min=1e-6
    )
    
    # è®­ç»ƒå†å²
    history = {
        'train_loss': [],
        'train_auc': [],
        'val_loss': [],
        'val_auc': [],
        'val_metrics': []
    }
    
    best_auc = 0.0
    best_epoch = 0
    
    # å¼€å§‹è®­ç»ƒ
    print("\n" + "=" * 70)
    print("ğŸ¯ å¼€å§‹è®­ç»ƒå¾ªç¯")
    print("=" * 70)
    
    for epoch in range(1, config.EPOCHS + 1):
        print(f"\n{'='*70}")
        print(f"Epoch {epoch}/{config.EPOCHS}")
        print(f"{'='*70}")
        
        start_time = time.time()
        
        # è®­ç»ƒ
        train_loss, train_auc = train_epoch(
            model, train_loader, criterion, optimizer, config.DEVICE
        )
        
        # éªŒè¯
        val_loss, val_metrics = evaluate(
            model, val_loader, criterion, config.DEVICE
        )
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # è®°å½•å†å²
        history['train_loss'].append(train_loss)
        history['train_auc'].append(train_auc)
        history['val_loss'].append(val_loss)
        history['val_auc'].append(val_metrics['auc'])
        history['val_metrics'].append(val_metrics)
        
        # æ‰“å°ç»“æœ
        epoch_time = time.time() - start_time
        print(f"\nğŸ“ˆ Epoch {epoch} ç»“æœ:")
        print(f"   Time: {epoch_time:.2f}s")
        print(f"   Train Loss: {train_loss:.4f}, Train AUC: {train_auc:.4f}")
        print(f"   Val Loss: {val_loss:.4f}, Val AUC: {val_metrics['auc']:.4f}")
        print(f"   Val Acc: {val_metrics['accuracy']:.4f}, Val F1: {val_metrics['f1']:.4f}")
        print(f"   Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_metrics['auc'] > best_auc:
            best_auc = val_metrics['auc']
            best_epoch = epoch
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_auc': best_auc,
                'config': config
            }, config.MODEL_SAVE_PATH)
            print(f"   âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜! (AUC: {best_auc:.4f})")
    
    # è®­ç»ƒå®Œæˆ
    print("\n" + "=" * 70)
    print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
    print("=" * 70)
    print(f"\nğŸ“Š æœ€ä½³ç»“æœ:")
    print(f"   Best Epoch: {best_epoch}")
    print(f"   Best Val AUC: {best_auc:.4f}")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_history(history)
    
    # æ‰“å°æœ€ç»ˆéªŒè¯é›†è¯¦ç»†æŒ‡æ ‡
    print(f"\nğŸ“‹ æœ€ç»ˆéªŒè¯é›†æŒ‡æ ‡:")
    final_metrics = history['val_metrics'][-1]
    for metric, value in final_metrics.items():
        print(f"   {metric.capitalize()}: {value:.4f}")
    
    return model, history


if __name__ == '__main__':
    model, history = train()
