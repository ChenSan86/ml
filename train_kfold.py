"""
KæŠ˜äº¤å‰éªŒè¯è®­ç»ƒæ¨¡å—
"""
import os
import time
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from tqdm import tqdm
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_fscore_support
import matplotlib.pyplot as plt
import seaborn as sns

from config import config
from model import create_model
from dataset import load_data, get_dataloaders
from train import set_seed, train_epoch, evaluate


def plot_kfold_results(fold_results, save_path='kfold_results.png'):
    """ç»˜åˆ¶KæŠ˜äº¤å‰éªŒè¯ç»“æœ"""
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    metrics = ['auc', 'accuracy', 'f1', 'precision']
    titles = ['AUC Score', 'Accuracy', 'F1 Score', 'Precision']

    for idx, (metric, title) in enumerate(zip(metrics, titles)):
        ax = axes[idx // 2, idx % 2]

        # æå–æ¯ä¸€æŠ˜çš„æŒ‡æ ‡
        values = [result[metric] for result in fold_results]
        folds = list(range(1, len(values) + 1))

        # ç»˜åˆ¶æŠ˜çº¿å›¾
        ax.plot(folds, values, marker='o', linewidth=2,
                markersize=8, label=f'{title}')

        # æ·»åŠ å¹³å‡çº¿
        mean_val = np.mean(values)
        ax.axhline(y=mean_val, color='r', linestyle='--', linewidth=2,
                   label=f'Mean: {mean_val:.4f}')

        # æ·»åŠ æ ‡å‡†å·®èŒƒå›´
        std_val = np.std(values)
        ax.fill_between(folds, mean_val - std_val, mean_val + std_val,
                        alpha=0.2, color='blue', label=f'Â±1 std: {std_val:.4f}')

        ax.set_xlabel('Fold', fontsize=12)
        ax.set_ylabel(title, fontsize=12)
        ax.set_title(f'{title} across Folds', fontsize=14, fontweight='bold')
        ax.legend(fontsize=10)
        ax.grid(True, alpha=0.3)
        ax.set_xticks(folds)

    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"\nğŸ“Š KæŠ˜ç»“æœå›¾å·²ä¿å­˜åˆ°: {save_path}")


def train_kfold(n_splits=5, epochs_per_fold=10):
    """
    KæŠ˜äº¤å‰éªŒè¯è®­ç»ƒ

    Args:
        n_splits: æŠ˜æ•°ï¼ˆé»˜è®¤5æŠ˜ï¼‰
        epochs_per_fold: æ¯ä¸€æŠ˜è®­ç»ƒçš„epochæ•°
    """
    print("=" * 80)
    print(f"ğŸš€ å¼€å§‹ {n_splits} æŠ˜äº¤å‰éªŒè¯")
    print("=" * 80)

    # è®¾ç½®éšæœºç§å­
    set_seed(config.SEED)
    print(f"\nâš™ï¸  é…ç½®ä¿¡æ¯: {config}")

    # åŠ è½½æ‰€æœ‰æ•°æ®
    print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    df = load_data([config.TRAIN_FILE_1, config.TRAIN_FILE_2])

    # åˆ›å»ºKæŠ˜åˆ†å‰²å™¨ï¼ˆä¿æŒæ ‡ç­¾åˆ†å¸ƒï¼‰
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True,
                          random_state=config.SEED)

    # å­˜å‚¨æ¯ä¸€æŠ˜çš„ç»“æœ
    fold_results = []
    fold_models = []

    print("\n" + "=" * 80)
    print(f"ğŸ“Š å¼€å§‹ {n_splits} æŠ˜è®­ç»ƒ")
    print("=" * 80)

    # KæŠ˜äº¤å‰éªŒè¯
    for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['label']), 1):
        print("\n" + "=" * 80)
        print(f"ğŸ”„ Fold {fold}/{n_splits}")
        print("=" * 80)

        # åˆ’åˆ†æ•°æ®
        train_df = df.iloc[train_idx].reset_index(drop=True)
        val_df = df.iloc[val_idx].reset_index(drop=True)

        print(f"\nğŸ“Š æ•°æ®åˆ’åˆ†:")
        print(f"   è®­ç»ƒé›†: {len(train_df):,} æ¡")
        print(f"   éªŒè¯é›†: {len(val_df):,} æ¡")
        print(f"   è®­ç»ƒé›†æ­£æ ·æœ¬æ¯”ä¾‹: {train_df['label'].mean():.2%}")
        print(f"   éªŒè¯é›†æ­£æ ·æœ¬æ¯”ä¾‹: {val_df['label'].mean():.2%}")

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader = get_dataloaders(
            train_df, val_df,
            batch_size=config.BATCH_SIZE,
            num_workers=config.NUM_WORKERS
        )

        # åˆ›å»ºæ¨¡å‹
        print(f"\nğŸ—ï¸  åˆ›å»º Fold {fold} æ¨¡å‹...")
        model = create_model()
        model = model.to(config.DEVICE)

        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.BCEWithLogitsLoss()
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config.LEARNING_RATE,
            weight_decay=config.WEIGHT_DECAY
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=epochs_per_fold, eta_min=1e-6
        )

        # è®°å½•æœ¬æŠ˜æœ€ä½³ç»“æœ
        best_auc = 0.0
        best_metrics = None

        # è®­ç»ƒæœ¬æŠ˜æ¨¡å‹
        print(f"\n{'='*80}")
        print(f"ğŸ¯ Fold {fold} è®­ç»ƒå¼€å§‹")
        print(f"{'='*80}")

        for epoch in range(1, epochs_per_fold + 1):
            print(f"\n--- Fold {fold}, Epoch {epoch}/{epochs_per_fold} ---")

            # è®­ç»ƒ
            train_loss, train_auc = train_epoch(
                model, train_loader, criterion, optimizer, config.DEVICE
            )

            # éªŒè¯
            val_loss, val_metrics = evaluate(
                model, val_loader, criterion, config.DEVICE
            )

            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()

            # æ‰“å°ç»“æœ
            print(f"Train Loss: {train_loss:.4f}, Train AUC: {train_auc:.4f}")
            print(f"Val Loss: {val_loss:.4f}, Val AUC: {val_metrics['auc']:.4f}, "
                  f"Val Acc: {val_metrics['accuracy']:.4f}, Val F1: {val_metrics['f1']:.4f}")

            # ä¿å­˜æœ¬æŠ˜æœ€ä½³æ¨¡å‹
            if val_metrics['auc'] > best_auc:
                best_auc = val_metrics['auc']
                best_metrics = val_metrics.copy()

                # ä¿å­˜æ¨¡å‹
                fold_model_path = f'best_model_fold{fold}.pth'
                torch.save({
                    'fold': fold,
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'best_auc': best_auc,
                    'metrics': best_metrics
                }, fold_model_path)
                print(f"   âœ… Fold {fold} æœ€ä½³æ¨¡å‹å·²ä¿å­˜! (AUC: {best_auc:.4f})")

        # è®°å½•æœ¬æŠ˜æœ€ä½³ç»“æœ
        print(f"\n{'='*80}")
        print(f"ğŸ“ˆ Fold {fold} æœ€ä½³ç»“æœ:")
        print(f"{'='*80}")
        for metric, value in best_metrics.items():
            print(f"   {metric.capitalize()}: {value:.4f}")

        fold_results.append(best_metrics)
        fold_models.append(f'best_model_fold{fold}.pth')

        # æ¸…ç†å†…å­˜
        del model, optimizer, scheduler
        torch.cuda.empty_cache()

    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    print("\n" + "=" * 80)
    print("ğŸ‰ KæŠ˜äº¤å‰éªŒè¯å®Œæˆ!")
    print("=" * 80)

    # è®¡ç®—å¹³å‡å’Œæ ‡å‡†å·®
    print("\nğŸ“Š æ€»ä½“ç»“æœç»Ÿè®¡:")
    print("=" * 80)

    results_df = pd.DataFrame(fold_results)

    print("\nå„æŠ˜è¯¦ç»†ç»“æœ:")
    print(results_df.to_string(index=False))

    print("\n\nå¹³å‡å€¼å’Œæ ‡å‡†å·®:")
    print("-" * 80)
    for metric in results_df.columns:
        mean_val = results_df[metric].mean()
        std_val = results_df[metric].std()
        print(f"{metric.capitalize():12s}: {mean_val:.4f} Â± {std_val:.4f}")

    # æ‰¾å‡ºæœ€ä½³æŠ˜
    best_fold = results_df['auc'].idxmax() + 1
    best_fold_auc = results_df['auc'].max()
    print(f"\nğŸ† æœ€ä½³æŠ˜: Fold {best_fold} (AUC: {best_fold_auc:.4f})")

    # ä¿å­˜ç»“æœ
    results_df['fold'] = range(1, n_splits + 1)
    results_df = results_df[['fold'] +
                            [col for col in results_df.columns if col != 'fold']]
    results_df.to_csv('kfold_results.csv', index=False)
    print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: kfold_results.csv")

    # ç»˜åˆ¶ç»“æœå›¾
    plot_kfold_results(fold_results)

    # é›†æˆé¢„æµ‹ï¼ˆå¯é€‰ï¼‰
    print("\n" + "=" * 80)
    print("ğŸ’¡ æç¤º:")
    print("=" * 80)
    print(f"1. å„æŠ˜æ¨¡å‹å·²ä¿å­˜ä¸º: best_model_fold1.pth ~ best_model_fold{n_splits}.pth")
    print(f"2. å¯ä»¥ä½¿ç”¨æœ€ä½³æŠ˜æ¨¡å‹ (Fold {best_fold}) è¿›è¡Œé¢„æµ‹")
    print(f"3. æˆ–è€…ä½¿ç”¨æ‰€æœ‰æ¨¡å‹çš„é›†æˆé¢„æµ‹ä»¥è·å¾—æ›´å¥½çš„æ³›åŒ–æ€§èƒ½")

    return results_df, fold_models


def ensemble_predict(fold_models, val_df):
    """
    ä½¿ç”¨æ‰€æœ‰æŠ˜çš„æ¨¡å‹è¿›è¡Œé›†æˆé¢„æµ‹

    Args:
        fold_models: å„æŠ˜æ¨¡å‹è·¯å¾„åˆ—è¡¨
        val_df: éªŒè¯æ•°æ®
    """
    print("\n" + "=" * 80)
    print("ğŸ”® é›†æˆé¢„æµ‹")
    print("=" * 80)

    from torch.utils.data import DataLoader
    from dataset import TextMatchDataset

    # å‡†å¤‡æ•°æ®
    val_dataset = TextMatchDataset(val_df, max_len=config.MAX_LEN)
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=False,
        num_workers=config.NUM_WORKERS,
        pin_memory=True
    )

    all_probs = []

    # åŠ è½½æ¯ä¸ªæ¨¡å‹å¹¶é¢„æµ‹
    for fold_idx, model_path in enumerate(fold_models, 1):
        print(f"\nåŠ è½½ Fold {fold_idx} æ¨¡å‹...")

        checkpoint = torch.load(
            model_path, map_location=config.DEVICE, weights_only=False)
        model = create_model()
        model.load_state_dict(checkpoint['model_state_dict'])
        model = model.to(config.DEVICE)
        model.eval()

        fold_probs = []
        with torch.no_grad():
            for batch in tqdm(val_loader, desc=f'Fold {fold_idx} Predicting'):
                query1 = batch['query1'].to(config.DEVICE)
                query2 = batch['query2'].to(config.DEVICE)

                logits = model(query1, query2)
                probs = torch.sigmoid(logits).cpu().numpy()
                fold_probs.extend(probs)

        all_probs.append(fold_probs)
        del model
        torch.cuda.empty_cache()

    # å¹³å‡æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
    ensemble_probs = np.mean(all_probs, axis=0)
    ensemble_preds = (ensemble_probs > 0.5).astype(int)

    # è®¡ç®—é›†æˆæŒ‡æ ‡
    labels = val_df['label'].values
    auc = roc_auc_score(labels, ensemble_probs)
    acc = accuracy_score(labels, ensemble_preds)
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, ensemble_preds, average='binary', zero_division=0
    )

    print("\n" + "=" * 80)
    print("ğŸ“ˆ é›†æˆé¢„æµ‹ç»“æœ")
    print("=" * 80)
    print(f"AUC: {auc:.4f}")
    print(f"Accuracy: {acc:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1: {f1:.4f}")

    return ensemble_probs, ensemble_preds


if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(description='KæŠ˜äº¤å‰éªŒè¯è®­ç»ƒ')
    parser.add_argument('--n_splits', type=int, default=5, help='æŠ˜æ•°')
    parser.add_argument('--epochs', type=int, default=10, help='æ¯æŠ˜è®­ç»ƒçš„epochæ•°')
    parser.add_argument('--ensemble', action='store_true', help='æ˜¯å¦è¿›è¡Œé›†æˆé¢„æµ‹')

    args = parser.parse_args()

    # KæŠ˜è®­ç»ƒ
    results_df, fold_models = train_kfold(
        n_splits=args.n_splits, epochs_per_fold=args.epochs)

    # é›†æˆé¢„æµ‹ï¼ˆå¯é€‰ï¼‰
    if args.ensemble:
        df = load_data([config.TRAIN_FILE_1, config.TRAIN_FILE_2])
        # ä½¿ç”¨å…¨éƒ¨æ•°æ®çš„ä¸€ä¸ªå­é›†ä½œä¸ºæµ‹è¯•
        from sklearn.model_selection import train_test_split
        _, test_df = train_test_split(
            df, test_size=0.1, random_state=config.SEED, stratify=df['label'])
        ensemble_probs, ensemble_preds = ensemble_predict(fold_models, test_df)
