"""
æ•°æ®åŠ è½½å’Œé¢„å¤„ç†æ¨¡å—
"""
import torch
import pandas as pd
import numpy as np
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from config import config


def load_data(file_paths):
    """åŠ è½½å¹¶åˆå¹¶å¤šä¸ªæ•°æ®æ–‡ä»¶"""
    dfs = []
    for file_path in file_paths:
        df = pd.read_csv(file_path, sep='\t', header=None, 
                        names=['query1', 'query2', 'label'])
        dfs.append(df)
    
    combined_df = pd.concat(dfs, ignore_index=True)
    print(f"âœ… åŠ è½½æ•°æ®å®Œæˆ: {len(combined_df):,} æ¡æ ·æœ¬")
    print(f"   æ­£æ ·æœ¬: {(combined_df['label']==1).sum():,} ({combined_df['label'].mean():.2%})")
    print(f"   è´Ÿæ ·æœ¬: {(combined_df['label']==0).sum():,}")
    
    return combined_df


def text_to_ids(text, max_len=60):
    """å°†æ–‡æœ¬è½¬æ¢ä¸ºIDåºåˆ—"""
    ids = [int(x) for x in str(text).split()]
    
    # æˆªæ–­
    if len(ids) > max_len:
        ids = ids[:max_len]
    
    # padding
    padding_len = max_len - len(ids)
    ids = ids + [0] * padding_len
    
    return ids


class TextMatchDataset(Dataset):
    """æ–‡æœ¬åŒ¹é…æ•°æ®é›†"""
    
    def __init__(self, df, max_len=60):
        self.query1 = df['query1'].values
        self.query2 = df['query2'].values
        self.labels = df['label'].values
        self.max_len = max_len
    
    def __len__(self):
        return len(self.labels)
    
    def __getitem__(self, idx):
        q1_ids = text_to_ids(self.query1[idx], self.max_len)
        q2_ids = text_to_ids(self.query2[idx], self.max_len)
        label = self.labels[idx]
        
        return {
            'query1': torch.tensor(q1_ids, dtype=torch.long),
            'query2': torch.tensor(q2_ids, dtype=torch.long),
            'label': torch.tensor(label, dtype=torch.float)
        }


def get_dataloaders(train_df, val_df, batch_size=256, num_workers=4):
    """åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨"""
    train_dataset = TextMatchDataset(train_df, max_len=config.MAX_LEN)
    val_dataset = TextMatchDataset(val_df, max_len=config.MAX_LEN)
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )
    
    return train_loader, val_loader


def prepare_data():
    """å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æ•°æ®"""
    # åŠ è½½æ•°æ®
    df = load_data([config.TRAIN_FILE_1, config.TRAIN_FILE_2])
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_df, val_df = train_test_split(
        df, 
        test_size=config.VAL_SPLIT,
        random_state=config.SEED,
        stratify=df['label']  # ä¿æŒæ ‡ç­¾åˆ†å¸ƒä¸€è‡´
    )
    
    print(f"\nğŸ“Š æ•°æ®åˆ’åˆ†:")
    print(f"   è®­ç»ƒé›†: {len(train_df):,} æ¡")
    print(f"   éªŒè¯é›†: {len(val_df):,} æ¡")
    
    return train_df, val_df


if __name__ == '__main__':
    # æµ‹è¯•æ•°æ®åŠ è½½
    train_df, val_df = prepare_data()
    train_loader, val_loader = get_dataloaders(train_df, val_df, batch_size=4)
    
    # æ‰“å°ä¸€ä¸ªbatch
    batch = next(iter(train_loader))
    print(f"\nğŸ” Batchç¤ºä¾‹:")
    print(f"   Query1 shape: {batch['query1'].shape}")
    print(f"   Query2 shape: {batch['query2'].shape}")
    print(f"   Labels shape: {batch['label'].shape}")
    print(f"   Query1[0]: {batch['query1'][0][:10]}...")
    print(f"   Label[0]: {batch['label'][0]}")
