"""
å¸¦æ³¨æ„åŠ›æœºåˆ¶çš„æ–‡æœ¬åŒ¹é…æ¨¡å‹
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from config import config


class AttentionLayer(nn.Module):
    """è‡ªæ³¨æ„åŠ›å±‚"""
    
    def __init__(self, hidden_dim):
        super(AttentionLayer, self).__init__()
        self.attention = nn.Linear(hidden_dim, 1, bias=False)
    
    def forward(self, lstm_output, mask=None):
        """
        Args:
            lstm_output: [batch_size, seq_len, hidden_dim]
            mask: [batch_size, seq_len] padding mask
        
        Returns:
            attended: [batch_size, hidden_dim]
            attention_weights: [batch_size, seq_len, 1]
        """
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        attention_scores = self.attention(lstm_output)  # [batch, seq_len, 1]
        
        # åº”ç”¨maskï¼ˆpaddingä½ç½®è®¾ä¸ºæå°å€¼ï¼‰
        if mask is not None:
            attention_scores = attention_scores.masked_fill(mask.unsqueeze(-1) == 0, -1e9)
        
        # Softmaxå½’ä¸€åŒ–
        attention_weights = torch.softmax(attention_scores, dim=1)
        
        # åŠ æƒæ±‚å’Œ
        attended = torch.sum(attention_weights * lstm_output, dim=1)
        
        return attended, attention_weights


class SiameseEncoderWithAttention(nn.Module):
    """å¸¦æ³¨æ„åŠ›çš„å­ªç”Ÿç¼–ç å™¨"""
    
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, dropout):
        super(SiameseEncoderWithAttention, self).__init__()
        
        # Embeddingå±‚
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        
        # BiLSTMå±‚
        self.lstm = nn.LSTM(
            embed_dim,
            hidden_dim // 2,
            num_layers=num_layers,
            batch_first=True,
            bidirectional=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        # æ³¨æ„åŠ›å±‚
        self.attention = AttentionLayer(hidden_dim)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        """
        x: [batch_size, seq_len]
        return: sentence_repr [batch_size, hidden_dim], attention_weights
        """
        # åˆ›å»ºpadding mask
        mask = (x != 0).float()  # [batch_size, seq_len]
        
        # Embedding
        embedded = self.embedding(x)
        embedded = self.dropout(embedded)
        
        # LSTM
        lstm_out, _ = self.lstm(embedded)
        lstm_out = self.dropout(lstm_out)
        
        # æ³¨æ„åŠ›æ± åŒ–
        sentence_repr, attention_weights = self.attention(lstm_out, mask)
        
        return sentence_repr, attention_weights


class InteractionLayer(nn.Module):
    """äº¤äº’å±‚"""
    
    def __init__(self, hidden_dim):
        super(InteractionLayer, self).__init__()
    
    def forward(self, repr1, repr2):
        # Cosine similarity
        cos_sim = F.cosine_similarity(repr1, repr2, dim=1, eps=1e-8)
        
        # Element-wise operations
        element_product = repr1 * repr2
        element_diff = torch.abs(repr1 - repr2)
        element_sum = repr1 + repr2
        
        # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
        interaction_features = torch.cat([
            cos_sim.unsqueeze(1),
            element_product,
            element_diff,
            element_sum
        ], dim=1)
        
        return interaction_features


class TextMatchModelWithAttention(nn.Module):
    """å¸¦æ³¨æ„åŠ›æœºåˆ¶çš„æ–‡æœ¬åŒ¹é…æ¨¡å‹"""
    
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, dropout):
        super(TextMatchModelWithAttention, self).__init__()
        
        # å…±äº«çš„å­ªç”Ÿç¼–ç å™¨ï¼ˆå¸¦æ³¨æ„åŠ›ï¼‰
        self.encoder = SiameseEncoderWithAttention(
            vocab_size, embed_dim, hidden_dim, num_layers, dropout
        )
        
        # äº¤äº’å±‚
        self.interaction = InteractionLayer(hidden_dim)
        
        # åˆ†ç±»å±‚
        interaction_dim = hidden_dim * 3 + 1
        self.classifier = nn.Sequential(
            nn.Linear(interaction_dim, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(256, 1)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.BatchNorm1d):
                nn.init.ones_(module.weight)
                nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Embedding):
                nn.init.uniform_(module.weight, -0.1, 0.1)
                if module.padding_idx is not None:
                    module.weight.data[module.padding_idx].zero_()
    
    def forward(self, query1, query2, return_attention=False):
        """
        query1, query2: [batch_size, seq_len]
        return_attention: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡
        """
        # ç¼–ç ä¸¤ä¸ªå¥å­
        repr1, attn1 = self.encoder(query1)
        repr2, attn2 = self.encoder(query2)
        
        # è®¡ç®—äº¤äº’ç‰¹å¾
        interaction_features = self.interaction(repr1, repr2)
        
        # åˆ†ç±»
        logits = self.classifier(interaction_features)
        
        if return_attention:
            return logits.squeeze(-1), attn1, attn2
        else:
            return logits.squeeze(-1)


def create_attention_model():
    """åˆ›å»ºå¸¦æ³¨æ„åŠ›çš„æ¨¡å‹å®ä¾‹"""
    model = TextMatchModelWithAttention(
        vocab_size=config.VOCAB_SIZE,
        embed_dim=config.EMBED_DIM,
        hidden_dim=config.HIDDEN_DIM,
        num_layers=config.NUM_LAYERS,
        dropout=config.DROPOUT
    )
    
    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"\nğŸ—ï¸  æ³¨æ„åŠ›æ¨¡å‹åˆ›å»ºå®Œæˆ:")
    print(f"   æ€»å‚æ•°é‡: {total_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"   æ–°å¢: æ³¨æ„åŠ›æœºåˆ¶")
    
    return model


if __name__ == '__main__':
    # æµ‹è¯•æ¨¡å‹
    model = create_attention_model()
    model.to(config.DEVICE)
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size = 4
    seq_len = 60
    query1 = torch.randint(1, 1000, (batch_size, seq_len)).to(config.DEVICE)
    query2 = torch.randint(1, 1000, (batch_size, seq_len)).to(config.DEVICE)
    
    # ä¸è¿”å›æ³¨æ„åŠ›
    outputs = model(query1, query2)
    print(f"\nğŸ” æ¨¡å‹æµ‹è¯•:")
    print(f"   Input shape: [{batch_size}, {seq_len}]")
    print(f"   Output shape: {outputs.shape}")
    print(f"   Sample outputs: {outputs[:3]}")
    
    # è¿”å›æ³¨æ„åŠ›æƒé‡
    outputs, attn1, attn2 = model(query1, query2, return_attention=True)
    print(f"\nğŸ“Š æ³¨æ„åŠ›æƒé‡:")
    print(f"   Attention1 shape: {attn1.shape}")
    print(f"   Attention2 shape: {attn2.shape}")
