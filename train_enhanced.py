"""
å¢å¼ºç‰ˆæ¨¡å‹è®­ç»ƒï¼šä½¿ç”¨ç‰¹å¾å·¥ç¨‹
"""
import os
import time
import torch
import torch.nn as nn
import numpy as np
from tqdm import tqdm
from sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_fscore_support
import matplotlib.pyplot as plt

from config import config
from model_enhanced import create_enhanced_model
from dataset import prepare_data, get_dataloaders
from train import set_seed, compute_metrics, plot_training_history


def train_epoch_enhanced(model, train_loader, criterion, optimizer, device):
    """è®­ç»ƒä¸€ä¸ªepochï¼ˆå¢å¼ºç‰ˆï¼‰"""
    model.train()
    total_loss = 0
    all_labels = []
    all_probs = []
    
    progress_bar = tqdm(train_loader, desc='Training')
    for batch in progress_bar:
        query1 = batch['query1'].to(device)
        query2 = batch['query2'].to(device)
        labels = batch['label'].to(device)
        
        # å‰å‘ä¼ æ’­ï¼ˆæ¨¡å‹å†…éƒ¨ä¼šæå–æ‰‹å·¥ç‰¹å¾ï¼‰
        optimizer.zero_grad()
        logits = model(query1, query2)
        
        # è®¡ç®—æŸå¤±
        loss = criterion(logits, labels)
        
        # åå‘ä¼ æ’­
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        # ç»Ÿè®¡
        total_loss += loss.item()
        probs = torch.sigmoid(logits).detach().cpu().numpy()
        all_labels.extend(labels.cpu().numpy())
        all_probs.extend(probs)
        
        # æ›´æ–°è¿›åº¦æ¡
        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})
    
    # è®¡ç®—å¹³å‡æŸå¤±å’ŒAUC
    avg_loss = total_loss / len(train_loader)
    auc = roc_auc_score(all_labels, all_probs)
    
    return avg_loss, auc


@torch.no_grad()
def evaluate_enhanced(model, val_loader, criterion, device):
    """è¯„ä¼°æ¨¡å‹ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    model.eval()
    total_loss = 0
    all_labels = []
    all_probs = []
    all_preds = []
    
    progress_bar = tqdm(val_loader, desc='Evaluating')
    for batch in progress_bar:
        query1 = batch['query1'].to(device)
        query2 = batch['query2'].to(device)
        labels = batch['label'].to(device)
        
        # å‰å‘ä¼ æ’­
        logits = model(query1, query2)
        loss = criterion(logits, labels)
        
        # ç»Ÿè®¡
        total_loss += loss.item()
        probs = torch.sigmoid(logits).cpu().numpy()
        preds = (probs > 0.5).astype(int)
        
        all_labels.extend(labels.cpu().numpy())
        all_probs.extend(probs)
        all_preds.extend(preds)
    
    # è®¡ç®—æŒ‡æ ‡
    avg_loss = total_loss / len(val_loader)
    metrics = compute_metrics(all_labels, all_preds, all_probs)
    
    return avg_loss, metrics


def train_enhanced():
    """å¢å¼ºç‰ˆå®Œæ•´è®­ç»ƒæµç¨‹"""
    print("=" * 80)
    print("ğŸš€ å¼€å§‹è®­ç»ƒå¢å¼ºç‰ˆæ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼ˆèåˆç‰¹å¾å·¥ç¨‹ï¼‰")
    print("=" * 80)
    
    # è®¾ç½®éšæœºç§å­
    set_seed(config.SEED)
    print(f"\nâš™ï¸  é…ç½®ä¿¡æ¯: {config}")
    
    # å‡†å¤‡æ•°æ®
    print("\nğŸ“‚ å‡†å¤‡æ•°æ®...")
    train_df, val_df = prepare_data()
    train_loader, val_loader = get_dataloaders(
        train_df, val_df,
        batch_size=config.BATCH_SIZE,
        num_workers=config.NUM_WORKERS
    )
    
    # åˆ›å»ºå¢å¼ºç‰ˆæ¨¡å‹
    print("\nğŸ—ï¸  åˆ›å»ºå¢å¼ºç‰ˆæ¨¡å‹...")
    model = create_enhanced_model()
    model = model.to(config.DEVICE)
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.BCEWithLogitsLoss()
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.LEARNING_RATE,
        weight_decay=config.WEIGHT_DECAY
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=config.EPOCHS, eta_min=1e-6
    )
    
    # è®­ç»ƒå†å²
    history = {
        'train_loss': [],
        'train_auc': [],
        'val_loss': [],
        'val_auc': [],
        'val_metrics': []
    }
    
    best_auc = 0.0
    best_epoch = 0
    
    # å¼€å§‹è®­ç»ƒ
    print("\n" + "=" * 80)
    print("ğŸ¯ å¼€å§‹è®­ç»ƒå¾ªç¯")
    print("=" * 80)
    
    for epoch in range(1, config.EPOCHS + 1):
        print(f"\n{'='*80}")
        print(f"Epoch {epoch}/{config.EPOCHS}")
        print(f"{'='*80}")
        
        start_time = time.time()
        
        # è®­ç»ƒ
        train_loss, train_auc = train_epoch_enhanced(
            model, train_loader, criterion, optimizer, config.DEVICE
        )
        
        # éªŒè¯
        val_loss, val_metrics = evaluate_enhanced(
            model, val_loader, criterion, config.DEVICE
        )
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # è®°å½•å†å²
        history['train_loss'].append(train_loss)
        history['train_auc'].append(train_auc)
        history['val_loss'].append(val_loss)
        history['val_auc'].append(val_metrics['auc'])
        history['val_metrics'].append(val_metrics)
        
        # æ‰“å°ç»“æœ
        epoch_time = time.time() - start_time
        print(f"\nğŸ“ˆ Epoch {epoch} ç»“æœ:")
        print(f"   Time: {epoch_time:.2f}s")
        print(f"   Train Loss: {train_loss:.4f}, Train AUC: {train_auc:.4f}")
        print(f"   Val Loss: {val_loss:.4f}, Val AUC: {val_metrics['auc']:.4f}")
        print(f"   Val Acc: {val_metrics['accuracy']:.4f}, Val F1: {val_metrics['f1']:.4f}")
        print(f"   Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_metrics['auc'] > best_auc:
            best_auc = val_metrics['auc']
            best_epoch = epoch
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_auc': best_auc,
                'config': config,
                'feature_mean': model.feature_mean,
                'feature_std': model.feature_std
            }, 'best_model_enhanced.pth')
            print(f"   âœ… æœ€ä½³å¢å¼ºæ¨¡å‹å·²ä¿å­˜! (AUC: {best_auc:.4f})")
    
    # è®­ç»ƒå®Œæˆ
    print("\n" + "=" * 80)
    print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
    print("=" * 80)
    print(f"\nğŸ“Š æœ€ä½³ç»“æœ:")
    print(f"   Best Epoch: {best_epoch}")
    print(f"   Best Val AUC: {best_auc:.4f}")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_history(history, save_path='training_history_enhanced.png')
    
    # æ‰“å°æœ€ç»ˆéªŒè¯é›†è¯¦ç»†æŒ‡æ ‡
    print(f"\nğŸ“‹ æœ€ç»ˆéªŒè¯é›†æŒ‡æ ‡:")
    final_metrics = history['val_metrics'][-1]
    for metric, value in final_metrics.items():
        print(f"   {metric.capitalize()}: {value:.4f}")
    
    # ä¸åŸºçº¿æ¨¡å‹å¯¹æ¯”
    print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”:")
    print(f"   åŸºçº¿æ¨¡å‹ AUC: 0.9718 (ä¸å«ç‰¹å¾å·¥ç¨‹)")
    print(f"   å¢å¼ºæ¨¡å‹ AUC: {best_auc:.4f} (å«ç‰¹å¾å·¥ç¨‹)")
    if best_auc > 0.9718:
        improvement = (best_auc - 0.9718) * 100
        print(f"   ğŸ‰ æå‡: +{improvement:.2f}%")
    
    return model, history


if __name__ == '__main__':
    model, history = train_enhanced()
